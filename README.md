# EvolSynth-API ğŸš€

**Advanced Synthetic Data Generation using LangGraph-based Evol-Instruct methodology**

Transform your documents into sophisticated evaluation datasets with intelligent question evolution, concurrent processing, and comprehensive quality assessment.

> **ğŸ”§ Recent Fix (2025-01):** Fixed critical evaluation scoring issue where quality metrics always showed 100%. The system now uses a research-backed numerical scoring approach (1-10 scale) that provides realistic and meaningful quality assessments. [Learn more about the fix â†’](#evaluation-scoring-system)

## ğŸ¯ What is EvolSynth?

EvolSynth implements the cutting-edge **Evol-Instruct methodology** using **LangGraph workflows** to generate high-quality synthetic evaluation data. Unlike simple question generators, EvolSynth creates progressively complex questions through three sophisticated evolution strategies:

### ğŸ§  Evolution Strategies

| Strategy | Complexity | Purpose | Example Transformation |
|----------|------------|---------|----------------------|
| **ğŸ¯ Simple Evolution** | Level 2 | Detail enhancement | "What is a loan?" â†’ "What are the specific eligibility requirements and application procedures for federal student loans?" |
| **ğŸŒ Multi-Context Evolution** | Level 3 | Cross-document synthesis | "What is financial aid?" â†’ "How do Pell Grant eligibility requirements compare with Direct Loan criteria across different academic programs?" |
| **ğŸ§  Reasoning Evolution** | Level 4 | Multi-step logical inference | "What affects loan amounts?" â†’ "If a student's dependency status changes mid-year, how would this impact their loan eligibility and disbursement schedule?" |

## ğŸ¨ Modal Documentation System

EvolSynth features a **comprehensive Modal Documentation system** that provides an integrated, user-friendly documentation experience directly within the application.

### ğŸ—ï¸ Modal Documentation Architecture

```mermaid
graph TD
    A[Click View Documentation] --> B[Modal Opens]
    B --> C[Header with Close Button]
    C --> D[Sidebar Navigation]
    C --> E[Main Content Area]
    
    D --> F[Overview Section]
    D --> G[Quick Start Section]
    D --> H[API Reference Section]
    D --> I[Examples Section]
    
    G --> J[Prerequisites Check]
    G --> K[Step-by-Step Guide]
    G --> L[Visual Progress]
    
    I --> M[Python Examples]
    I --> N[JavaScript Examples]
    I --> O[cURL Examples]
    
    M --> P[Copy to Clipboard]
    N --> P
    O --> P
    
    style B fill:#e1f5fe,stroke:#01579b,stroke-width:2px
    style D fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    style P fill:#e8f5e8,stroke:#1b5e20,stroke-width:2px
```

### ğŸ¯ Modal Documentation Features

#### **ğŸ“± Integrated Experience**
- **Rich Modal Interface**: Beautiful overlay with backdrop blur
- **Responsive Design**: Works seamlessly on all devices
- **Brand Consistency**: Matches application design language
- **No Redirects**: Stays within the application context

#### **ğŸ§­ Interactive Navigation**
- **4 Comprehensive Sections:**
  - ğŸ“– **Overview**: Features, benefits, and evolution strategies
  - âš¡ **Quick Start**: Step-by-step setup with prerequisites
  - ğŸ”§ **API Reference**: Complete endpoint documentation
  - ğŸ’» **Examples**: Multi-language code samples

#### **ğŸ“‹ Developer-Friendly Features**
- **Copy-to-Clipboard**: One-click copying for all code examples
- **Multiple Languages**: Python, JavaScript, cURL examples
- **Visual Feedback**: Animated confirmations and progress indicators
- **Real Examples**: Ready-to-run code snippets

#### **ğŸ¨ Enhanced User Experience**
- **Visual Step Numbers**: Numbered progress indicators
- **Prerequisites Checklist**: Clear setup requirements
- **Expected Responses**: What to expect from each API call
- **Dual-Path Guidance**: Interface vs API usage options

### ğŸ’» Code Examples Available

**Python Integration:**
```python
import requests

response = requests.post("http://localhost:8000/generate", json={
    "documents": [{"content": "...", "metadata": {...}}],
    "settings": {"execution_mode": "concurrent", ...}
})
```

**JavaScript/Node.js:**
```javascript
const response = await fetch('http://localhost:8000/generate', {
  method: 'POST',
  headers: { 'Content-Type': 'application/json' },
  body: JSON.stringify({...})
});
```

**cURL Commands:**
```bash
curl -X POST "http://localhost:8000/generate" \
  -H "Content-Type: application/json" \
  -d '{...}'
```

### ğŸ”„ Documentation Options

The system provides **flexible documentation access**:

1. **Modal Documentation** (Default): Integrated experience within the app
2. **External Swagger Docs**: Full FastAPI documentation at `/docs`
3. **Easy Switching**: Toggle between modes with simple code changes

### ğŸª Benefits Over Traditional Documentation

| Feature | Modal Documentation | External Docs |
|---------|-------------------|---------------|
| **User Context** | âœ… Stays in app | âŒ Redirects away |
| **Design Consistency** | âœ… Matches brand | âŒ Generic styling |
| **Mobile Experience** | âœ… Optimized | âš ï¸ Basic |
| **Learning Path** | âœ… Guided journey | âŒ Reference-focused |
| **Code Examples** | âœ… Multi-language | âœ… Auto-generated |
| **Copy-to-Clipboard** | âœ… Custom implementation | âš ï¸ Limited |

## ğŸš€ Quick Start

### Prerequisites
- Backend running on `http://localhost:8000`
- OpenAI API key configured
- LangSmith API key for monitoring (optional)

### 1. Health Check
```bash
curl http://localhost:8000/health
```

### 2. Access Documentation
- **Frontend**: Click "View Documentation" button
- **API Docs**: Visit `http://localhost:8000/docs`

### 3. Generate Data
- Use the beautiful frontend interface
- Or make direct API calls programmatically

## ğŸ“Š Performance Features

- **ğŸ”„ Concurrent Execution**: 3x faster generation through LangGraph workflows
- **ğŸ“ˆ Real-time Monitoring**: LangSmith integration for evaluation tracking
- **ğŸšï¸ Quality Control**: Built-in LLM-as-judge evaluation
- **ğŸ”§ Flexible Configuration**: Customizable evolution parameters

## ğŸ› ï¸ Architecture

```
ğŸ“ EvolSynth-API/
â”œâ”€â”€ ğŸ¨ frontend/           # Next.js frontend with Modal Documentation
â”œâ”€â”€ âš™ï¸ api/                # FastAPI backend with LangGraph workflows
â”œâ”€â”€ ğŸ§ª tests/              # Comprehensive test suite
â””â”€â”€ ğŸ“š docs/               # Additional documentation
```

## ğŸ“ˆ Quality Assessment

EvolSynth includes **comprehensive quality assessment** with:
- **Question Quality**: Clarity, specificity, educational value
- **Answer Accuracy**: Correctness and completeness
- **Evolution Effectiveness**: Cognitive complexity achievement
- **LangSmith Monitoring**: Real-time evaluation tracking

## ğŸ¤ Contributing

We welcome contributions! The project is built with:
- **ğŸ—ï¸ FastAPI**: High-performance API framework
- **ğŸ”— LangChain**: Robust LLM integration
- **ğŸŒ LangGraph**: Advanced workflow orchestration
- **âš›ï¸ Next.js**: Modern React frontend
- **ğŸ“Š LangSmith**: Comprehensive monitoring

## ğŸ§® Evaluation Scoring System

### **The 100% Quality Metrics Issue (Fixed)**

**Problem:** Previous versions used binary LLM-as-judge evaluation that consistently returned 100% quality scores due to response bias.

**Root Cause:** Simple string matching (`"GOOD" â†’ 1.0, else â†’ 0.0`) caused inflated metrics, a well-documented issue in LLM evaluation research.

### **New Numerical Scoring System**

**Solution:** Research-backed numerical evaluation with 1-10 scale scoring:

```python
# Enhanced Evaluation Criteria
Quality Assessment:
â”œâ”€â”€ Question Quality (1-10 scale)
â”‚   â”œâ”€â”€ Clarity and specificity
â”‚   â”œâ”€â”€ Appropriate complexity
â”‚   â”œâ”€â”€ Educational value
â”‚   â””â”€â”€ Grammatical correctness
â”œâ”€â”€ Answer Accuracy (1-10 scale)
â”‚   â”œâ”€â”€ Factual correctness
â”‚   â”œâ”€â”€ Completeness
â”‚   â”œâ”€â”€ Clarity and coherence
â”‚   â””â”€â”€ Relevance to question
â””â”€â”€ Evolution Effectiveness (1-10 scale)
    â”œâ”€â”€ Cognitive complexity achieved
    â”œâ”€â”€ Evolution type appropriateness
    â””â”€â”€ Meta-cognitive requirements
```

**Features:**
- **Structured Prompts**: Clear evaluation criteria and output format
- **Robust Extraction**: Multiple regex patterns and fallback methods
- **Score Distributions**: Mean, min, max, standard deviation tracking
- **Realistic Results**: Typical scores 60-85% instead of artificial 100%

**Academic Backing:** Based on recent research addressing LLM-as-judge limitations:
- *"No Free Labels: Limitations of LLM-as-a-Judge Without Human Grounding"*
- *"Evaluating Scoring Bias in LLM-as-a-Judge"*
- *"Judging the Judges: Evaluating Alignment and Vulnerabilities in LLMs-as-Judges"*

## ğŸ“„ License

Built with â¤ï¸ for the AI community. Based on the Evol-Instruct methodology from WizardLM research.

---

**Ready to evolve your data?** ğŸš€ Start generating sophisticated synthetic evaluation datasets that push the boundaries of AI system assessment!
